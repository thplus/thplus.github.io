---
title: Pre-Trained Model
date: 2025-03-04
categories: [Today I Learn, 6th Week]
tags: [python, tensorflow, pytorch]
math: true
---

## 한 줄 정리

||한 줄 정리|비고|
|--|--|--|
|[Pre-trained Model](#pre-trained-model사전-훈련-모델)|대규모 데이터셋에서 미리 학습되어 이미 훈련이 끝난 모델|모델을 처음부터 구축하지 않아도 되어 비용을 절약할 수 있다.|
|[Transfer Learning](#transfer-learning)|사전 훈련된 모델을 그대로 사용하거나 추가 튜닝하여 새로운 문제에 적용하는 것||
|[Fine Tuning](#fine-tuning미세-조정)|Pre-trained model을 새로운 데이터셋에 맞게 재훈련하는 과정|대규모 데이터셋에서 미리 학습된 파라미터 등을 활용하고 새로운 데이터셋에 맞게 파라미터를 조정하기 위해|
|[Feature Extraction](#feature-extraction특징-추출)|Pre-trained Model의 파라미터는 그대로 사용하고 분류 layer만 새로 학습하는 것|새로운 데이터셋에서도 특징 추출은 유용할 수 있음|
|[Overfitting](#overfitting과적합)|모델이 학습 데이터에 대해 과도하게 학습하여 일반화 성능이 떨어지는 현상||
|[Underfitting](#underfitting과소적합)|모델이 학습 데이터를 충분히 학습하지 못하여 예측 성능이 떨어지는 현상||

## Model Architecture(모델 구성)
- Model Architecture는 신경망에서 각 레이어의 구성, 연결 방식, 활성화 함수, 입력 및 출력 형태 등 전체 모델의 구조이다. 특히 딥러닝 모델에서 다양한 레이어가 어떻게 결합하여 데이터를 처리하고 결과를 도출하는지 정의하는 중요한 개념이다.

- 딥러닝 모델은 기본적으로 학습 과정을 통해 찾아낸 최적 weight들의 집합니다.

### Epoch
- Training Data와 Validation Data로 나눈 데이터로 학습을 한다고 가정하면 아래와 같은 과정으로 Epoch이 수행된다.
    1. Training Data마다 각각 다른 비선형 그래프가 그려지고 Learning Rate를 조정하여 학습을 진행하면서 최적의 Loss값을 찾는다.
    2. 모든 Training Data에 대한 학습이 완료되면 모델이 학습한 내용을 바탕으로 Validation Data에 대한 예측을 수행한다.
    3. Validation Data의 Label과 모델의 예측 값을 비교하여 Loss값과 Accuracy를 계산하여 해당 Epoch에 대한 학습과 검증 결과로 Loss와 Accuracy 값이 출력된 후 학습된 모델이 저장된다.

### Loss(손실)
- 실제 값과 예측 값의 차이
 $|Y - \hat Y|$ 
 로 정의할 수 있다.
- Loss가 0에 가까울수록 좋다. 그러나 Loss가 낮다고 무조건 좋은 것은 아니며 Overfitting의 경우 Loss가 낮아도 테스트 데이터의 성능이 떨어질 수 있다.

### Accuracy(정확도)
- 전체 $n$개의 샘플에 대해 각 샘플 $i$에 대해 모델의 예측과 실제 값이 일치하는 정도<br/>
- 전체 샘플

    $$
     \text{Accuracy} = \frac{1}{n}\sum_{i=1}^{n} \mathbf{1}\{y_i = \hat{y}_i\}
    $$
     <br/>

- 이진 분류

    $$
    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
    $$

## Pre-trained Model(사전 훈련 모델)
- 사전 훈련 모델은 이미 훈련이 끝난 모델 또는 모델 구성요소를 말하며 이 모델은 대규모 데이터셋에서 미리 학습되어 특정한 작업에 필요한 특성을 이미 학습한 상태이다.

- 사전 훈련 모델을 사용하는 이유는 인공지능 모델을 처음부터 구축하지 않아도 되어 시간과 자원을 절약하며 신속하게 활용할 수 있기 때문이다.

## ResNet(Residual Network)
- ResNet은 Neural Network에서 발생하는 기울기 소실 문제를 해결하기 위해 Residual Connections을 도입한 사전 훈련 모델이다.<br/>
Residual Connections(Skip connection)을 통해 깊은 네트워크에서도 Vanishing Gradient가 현저히 줄어든 상태로 학습을 진행할 수 있으며 ResNet은 50, 101, 152층 등 다양한 깊이의 모델로 구성된다.

- ResNet은 매우 깊은 신경망에서도 기울기 소실 문제를 효과적으로 해결하여 더 높은 정확도와 성능을 제공하는 딥러닝 모델을 구축할 수 있다.

## VGG16
- Visual Geometry Group이라는 영국 옥스포드 대학교의 연구그룹이 제안한 CNN 구조로 13개 층의 Conv Layer와 3개의 Fully-Connected Layer를 합친 CNN 구조이다.

- VGG16은 깊고 구조화된 CNN 구조의 사전 학습 모델로 주로 이미지 인식과 분류 작업에 사용되는 사전 훈련 모델이다. VGG16은 심층 신경망의 설계를 단순화하면서도 강력한 성능을 유지하는데 초점을 맞춘 모델로 모든 합성곱 계층에서 3x3 크기의 필터를 사용하여 작은 수용영역으로도 복잡한 특징을 효과적으로 학습할 수 있도록 설계되었다.

- VGG16은 ImageNet 데이터셋으로 사전 훈련된 가중치를 제공하여 Transfer Learning에도 효과적으로 확용될 수 있다. 138M개의 파라미터를 가지는 대규모 모델이므로 연산량과 메모리 사용량이 크다는 단점이 있다.

## Transfer Learning
- Transfer Learning은 사전 훈련된 모델을 그대로 사용하거나 추가 튜닝하여 새로운 문제에 적용함으로써 학습 시간을 단축하고 성능을 향상시키는 머신 러닝 기법이다. Transfer Learning 기법을 사용하면 대규모 데이터셋 또는 특정 과제에서 사전 훈련된 네트워크가 추출해 낸 구수준 특성(Feature)이나 모델 가중치(Weight)를 새로운 과제에 적용함으로써 학습시간을 단축하고 모델 성능을 개선할 수 있게 된다.

- 전이 학습에는 주로 3가지 기법 (Fine-Tuning, Feature Extraction, Zero-Shot Learning)이 있다.

### Fine-tuning(미세 조정)
- 사전 훈련된 모델의 전체 혹은 일부 계층을 새로운 데이터셋에 맞게 재훈련하여 최적화하는 과정이다. 이미 학습된 가중치와 패턴을 활용하면서도 새로운 문제의 특성을 반영하기 위해 미세한 조정 과정을 거친다.

- 이미 학습된 가중치와 패턴을 활용하면서도 새로운 문제의 특성을 반영하기 위해 미세한 조정 과정을 거친다. 이 기법은 학습 자원이 많이 들 수 있지만 데이터셋 특유의 패턴을 보다 깊이 있게 반영하므로 더 높은 성능을 얻을 가능성이 크다.

1. Full Fine Tuning<br/>
    모델의 모든 파라미터를 새로운 데이터셋에 맞추어 학습화는 과정이다. 모델이 새로운 작업에 완전히 적응하도록 하기 때문에 가장 높은 성능을 기대할 수 있으나 많은 데이터와 계산 자원이 필요할 수 있으며 학습 시간이 오래 걸릴 수 있다.
    - 단계<br/>
        사전 학습된 모델 불러오기 $\rightarrow$ 모델의 구조 변경 (새로운 분류 레이어 추가) $\rightarrow$ 모든 레이어를 동결 해제 $\rightarrow$ 모델 컴파일 $\rightarrow$ 전체 모델 학습 $\rightarrow$ 모델 평가

2. Partial Fine Tuning<br/>
    모델의 일부 파라미터만 조정하는 방법이다. 모델의 상위 층(layer)만 미세 조정하고 하위 층은 그대로 두는 방식이다. 이렇게 하면 학습 시간이 단축되고 과적합(Overfitting) 문제를 줄일 수 있다.
    - 단계<br/>
        사전 학습된 모델 불러오기 $\rightarrow$ 모델의 구조 변경 (새로운 분류 레이어 추가) $\rightarrow$ 하위 레이어를 동결하고 상위 레이어만 동결 해제 $\rightarrow$ 모델 컴파일 $\rightarrow$ 일부 레이어 학습 $\rightarrow$ 모델 평가

3. 단계적 Fine Tuning <br/>
    먼저 모델의 상위 레이어만 재학습하고 그 다음 단계에서 하위 레이어까지 포함하여 재학습하는 접근법이다. 점진적으로 모델을 최적화하는데 사용된다.
    - 단계<br/>
        사전 학습된 모델 불러오기 $\rightarrow$ 모델의 구조 변경 (새로운 분류 레이어 추가) $\rightarrow$ 기본 레이어를 동결 $\rightarrow$ 모델 컴파일 $\rightarrow$ 상위 레이어 학습 $\rightarrow$ 하위 레이어 동결 해제 및 상위 레이어 일부 동결 유지 $\rightarrow$ 모델 재컴파일 및 전체 모델 학습 $\rightarrow$ 모델 평가

4. 하이브리드 방법<br/>
    여러 Fine Tuning 방법을 결합하여 사용하는 것으로 예를 들어 일부 레이어 Fine Tuning과 단계적 Fine Tuning을 결합하여 모델을 최적화한다.
    - 단계<br/>
        사전 학습된 모델 불러오기 $\rightarrow$ 모델의 구조 변경 (새로운 분류 레이어 추가) $\rightarrow$ 하위 레이어를 동결하고 상위 레이어만 동결 해제 $\rightarrow$ 모델 컴파일 $\rightarrow$ 상위 레이어 학습 $\rightarrow$ 일부 하위 레이어 동결 해제 $\rightarrow$ 모델 재컴파일 및 전체 모델 학습 $\rightarrow$ 모델 평가

### Feature Extraction(특징 추출)
- 사전 훈련된 모델이 지닌 중간 계층의 가중치를 그대로 고정하여 특징 추출기(Feature Extractor)로 활용하는 방식이다. 그 후 마지막 분류 레이어만 새로 학습한다. 사전 훈련된 모델이 가진 광범위한 저수준 ~ 고수준 특징을 재활용하므로 데이터셋 규모가 크지 않아도 빠른 시간 내에 괜찮은 성능을 낼 수 있다.

- 새로운 문제와 사전 훈련 모델이 학습된 주제(도메인)가 유사할수록 특징 추출의 효율이 높다.

- Feature Extraction을 사용하는 이유는 적은 리소스로도 높은 성능을 끌어 올릴 수 있기 때문이다. Feature Extraction 방식은 사전 훈련된 모델의 중간 계층이 이미 학습한 범용적이고 고도화된 특징을 재활용한다는 점에서 매우 효과적인 전이 학습 접근법이다.

### Zero-Shot Learning(제로샷 학습)
- 학습에 한 번도 등장하지 않은 클래스에 대해서도 모델이 예측을 수행할 수 있도록 하는 전이 학습 기법니다. 모델이 사전에 개념(Concept) 정보나 멀티모달(텍스트$\cdot$이미지) 표현을 학습해두고 이후 새로운 클래스가 등장했을 때, 유사도나 개념 연결로 그 클래스를 식별한다.

- 데이터가 없는 클래스에 대해서도 추론할 수 있어 확장성이 높고 희귀한 상황이나 긴급상황 분류 등에 활용될 수 있다.

- Zero-Shot Learning은 학습 데이터에 없는 새로운 클래스나 작업에도 추가학습 없이 즉시 대응하기 위해서 사용한다. 일반적인 머신 러닝이나 딥러닝 기법은 훈련 데이터에 등장한 클래스에 대해서만 예측 성능을 발휘한다는 한계를 가진다. 하지만 현실 세계에서는 학습 데이터로 준비되지 않은 수 많은 상황이나 새로운 분류 항목이 끊임없이 등장하기 때문에 추가적인 데이터 수집이나 재학습이 지속적으로 요구된다.

## Model Comparison(모델 비교)
- 여러 기계학습 모델의 성능을 평가하고 비교해 특정 문제에 대해 가장 효과적인 모델을 선택하는 과정을 말한다.

- 다양한 모델 중에서 성능이 가장 우수한 모델을 선택하여 주어진 문제를 최적의 방식으로 해결하고 예측 정확도를 극대화하기 위해서 사용한다.

## Overfitting(과적합)
- 학습 데이터를 과하게 학습해 실제 데이터에 대해서 오차가 증가하게 되는 것으로 모델이 학습 데이터의 노이즈나 특이한 패턴까지도 학습하여 일반적인 패턴을 제대로 학습하지 못하게 되기 때문에 발생한다.

- Overfitting의 발생 이유

    |이유|설명|
    |--|--|
    |모델 복잡성|너무 많은 파라미터나 복잡한 구조를 가진 모델은 학습 데이터에 과적합되기 쉽다.|
    |데이터 양의 부족|학습 데이터가 충분하지 않으면 모델이 데이터를 외우게 되어 과적합이 발생할 수 있다.|
    |노이즈 포함 학습|데이터에 노이즈가 많을 경우 모델이 노이즈까지 학습하여 새로운 데이터에 대한 성능이 떨어진다.|

- Overfitting은 모델이 새로운 데이터에서도 좋은 성능을 발휘할 수 있도록 일반화 성능을 높이기 위해서 방지해야할 필요가 있다. 과적합된 모델은 학습 데이터에서는 높은 성능을 보일 수 있지만 실제로 사용될 새로운 데이터에 대해서는 성능이 크게 떨어질 수 있다.

- Overfitting 방지의 중요성

    |항목|설명|
    |--|--|
    |일반화 능력|모델이 새로운 데이터에 대해서도 좋은 성능을 유지하게 함|
    |신뢰성|다양한 데이터 상황에서도 일관된 성능을 보장|
    |적용 가능성|실제 환경에서의 예측 및 분석의 정확도 향상|
    |비용 절감|잘 일반화 된 모델은 데이터 수집 및 처리 비용을 줄일 수 있음|

- Overfitting 방지 방법

    |항목|설명|
    |--|--|
    |드롭아웃(DropOut)|학습 과정에서 무작위로 뉴런을 끔으로써 과적합을 방지|
    |데이터 증강(Data Augmentation)|학습 데이터를 늘려서 다양한 상황에 대한 모델의 적응력을 높임|
    |정규화(Regularization)|L1, L2 정규화를 사용하여 큰 가중치 값을 가지지 않도록 함|
    |교차 검증(Cross-Validation)|데이터를 여러 번 나누어 검증하여 모델의 일반화 성능을 평가|
    |조기 종료(Early Stopping)|검증 데이터의 성능이 향상되지 않으면 학습을 조기 종료|
    |앙상블(Ensemble) 기법|여러 모델을 결합하여 예측 성능을 높임|

## Underfitting(과소적합)
- Underfitting은 모델이 데이터의 복잡성을 충분히 학습하지 못해 훈련 데이터와 새로운 데이터 모두에서 낮은 성능을 보이는 것으로 모델이 학습 과정에서 필요한 패턴이나 규칙을 제대로 파악하지 못한 채, 너무 단순화된 (혹은 충분히 훈련되지 않은) 형태로 머물러 있다는 뜻이다.

- 모델이 제대로 학습하지 못한다면 아무리 많은 데이터를 투입하거나 복잡한 아키텍처를 사용해도 원하는 성능을 얻기 어렵다. Underfitting에 대해 공부해야 모델이 학습 데이터의 본질적인 패턴을 충분히 학습하도록 설계$\cdot$최적화할 수 있다.

- Underfitting 방지 방법

    |항목|방법|
    |--|--|
    |모델 구조 개선|더 깊거나 복잡한 모델 사용, 비선형 활성화 함수 활용, 적절한 네트워크 아키텍처 선택|
    |데이터 활용 최적화|더 많은 데이터 확보, Data Augmentation(데이터 증강)기법 활용, 데이터 전처리 개선|
    |학습 설정 조정|적절한 학습률 설정, 충분한 학습 반복(Epoch 증가), 적절한 최적화 기법 선택|

## 오늘의 회고
- Pre-Trained Model을 중심으로 모델을 사용하는 과정을 학습하였다. 현대 AI Engineer의 기본 소양은 잘 정제되어 있는 Model을 이용하는 것이다.